{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AK7Qd4TExQr"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cskyan/lecture-bionlp/blob/master/notebooks/bionlp_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH9M0MS0FKWt"
      },
      "source": [
        "# Hands-on Tasks for Biomedical Text Mining (BioNLP)\n",
        "  - Basic Tasks\n",
        "    * Tokenization\n",
        "    * POS Tagging, Lemmatization, Stemming\n",
        "    * Stop Words\n",
        "  - Advanced Tasks\n",
        "    * NER\n",
        "    * Document Classification\n",
        "  - Neural Network\n",
        "    * Word Embedding\n",
        "    * Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1Wgd63gQxEd"
      },
      "source": [
        "# Data\n",
        "  - [PubMed](https://github.com/cskyan/lecture-bionlp-intro/blob/master/data/pubmed_samples.csv)\n",
        "  - [BLUE](https://github.com/ncbi-nlp/BLUE_Benchmark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYZIqb5DQqER"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "url = 'https://github.com/cskyan/lecture-bionlp/blob/master/data/pubmed_samples.csv?raw=true'\n",
        "pubmed_df = pd.read_csv(url,index_col='pmid')\n",
        "paragraph = pubmed_df.iloc[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg8tb3-Tl3f_"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/ncbi-nlp/BLUE_Benchmark/releases/download/0.1/data_v0.2.zip\n",
        "!unzip data_v0.2.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Nmd2QRmrVWT"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/cskyan/lecture-bionlp/blob/master/data/BC5CDR.zip?raw=true -O BC5CDR.zip\n",
        "!unzip BC5CDR.zip -d data/BC5CDR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PKeNac8Y1q-"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy-InbxHYy27"
      },
      "outputs": [],
      "source": [
        "!pip install ftfy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0_ScyLWY_Rg"
      },
      "outputs": [],
      "source": [
        "import ftfy\n",
        "\n",
        "pubmed_df.text = pubmed_df.text.apply(ftfy.fix_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3iqsfDm62zv"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDbkBt6b7RSd"
      },
      "source": [
        "## [NLTK](https://www.nltk.org/api/nltk.tokenize.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR4GYytf9OSD"
      },
      "source": [
        "###  Sentence segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvM2gbmzFGAL"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.sent_tokenize(paragraph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V9VJNqO8CWU"
      },
      "outputs": [],
      "source": [
        "# Download the model\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# Construct the tokenizer\n",
        "punkt_sent_tknzr = nltk.data.load('tokenizers/punkt/english.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3LXqzAK8Qnn"
      },
      "outputs": [],
      "source": [
        "punkt_sent_tknzr.tokenize(paragraph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3vfNYE0MAQQ"
      },
      "source": [
        "### Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlFd7fjkMMzk"
      },
      "outputs": [],
      "source": [
        "[' ## '.join(nltk.word_tokenize(sent)) for sent in nltk.sent_tokenize(paragraph)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqffDwwlYLaE"
      },
      "source": [
        "#### Penn Treebank Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhWAmyGpYNu4"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer\n",
        "treebank_tknzr = TreebankWordTokenizer()\n",
        "[' ## '.join(treebank_tknzr.tokenize(sent)) for sent in nltk.sent_tokenize(paragraph)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nCRhFHfMNZ4"
      },
      "source": [
        "### Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le1azQ7qPw8S"
      },
      "source": [
        "#### Stanford CoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVN7Imk_POgI"
      },
      "outputs": [],
      "source": [
        "# Deprecated\n",
        "# from nltk.parse import corenlp\n",
        "# stanford_tknzr = corenlp.CoreNLPParser(url=CORENLP_URL)\n",
        "# [' '.join(sent.leaves()) for sent in stanford_tknzr.parse_text(paragraph[:258])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEVLmVfoSBen"
      },
      "outputs": [],
      "source": [
        "# Install stanza and Stanford CoreNLP\n",
        "\n",
        "!pip install stanza\n",
        "import os, stanza\n",
        "corenlp_dir = './corenlp'\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir\n",
        "stanza.install_corenlp(dir=corenlp_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MiST7E7P90z"
      },
      "outputs": [],
      "source": [
        "CORENLP_URL = 'http://localhost:9001'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiM7CEvlDPyt"
      },
      "outputs": [],
      "source": [
        "# Start Server\n",
        "# !killall java\n",
        "from stanza.server import CoreNLPClient\n",
        "client = CoreNLPClient(\n",
        "    # annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'ner'],\n",
        "    annotators=['tokenize', 'ssplit'], \n",
        "    memory='4G', \n",
        "    endpoint=CORENLP_URL,\n",
        "    be_quiet=True)\n",
        "client.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sqnj7ZprObPk"
      },
      "outputs": [],
      "source": [
        "# [' '.join([tkn.word for tkn in sent.token]) for sent in client.annotate(paragraph).sentence]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVfF4ECuXKuV"
      },
      "outputs": [],
      "source": [
        "# Remember to close the server when finished\n",
        "client.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0RgJ8XlXzLm"
      },
      "outputs": [],
      "source": [
        "# Or use the with statement\n",
        "with CoreNLPClient(\n",
        "    # annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'ner'],\n",
        "    annotators=['tokenize', 'ssplit'], \n",
        "    memory='4G', \n",
        "    endpoint=CORENLP_URL,\n",
        "    be_quiet=True) as client:\n",
        "    print([' '.join([tkn.word for tkn in sent.token]) for sent in client.annotate(paragraph).sentence])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_12cfUMZRGR"
      },
      "source": [
        "## [SpaCy](https://spacy.io)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DANHpVwnVV_"
      },
      "outputs": [],
      "source": [
        "# !pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3szXXt7aUGV"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "spacy.prefer_gpu()\n",
        "spacy_nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmhcr055mgsE"
      },
      "outputs": [],
      "source": [
        "[' ## '.join([word.text for word in sent]) for sent in spacy_nlp(paragraph, disable=['entity']).sents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27BO5ZWgdYFt"
      },
      "source": [
        "### [SciSpaCy](allenai.github.io/scispacy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoRAoe6Tdoou"
      },
      "outputs": [],
      "source": [
        "!pip install scispacy\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0gIBWT2douR"
      },
      "outputs": [],
      "source": [
        "import scispacy\n",
        "scispacy.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxVY4qn3jXW8"
      },
      "outputs": [],
      "source": [
        "import scispacy\n",
        "import spacy\n",
        "\n",
        "scispacy_nlp = spacy.load(\"en_core_sci_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIo5jiOGmZNN"
      },
      "outputs": [],
      "source": [
        "[' ## '.join([word.text for word in sent]) for sent in scispacy_nlp(paragraph).sents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77OBNmfYcD7W"
      },
      "source": [
        "## [Stanza](https://stanfordnlp.github.io/stanza/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAxEMAv8cam3"
      },
      "outputs": [],
      "source": [
        "!pip install stanza\n",
        "USE_GPU = True\n",
        "import stanza\n",
        "stanza.download('en')\n",
        "stanza_nlp = stanza.Pipeline('en', processors='tokenize', use_gpu=USE_GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgCwd8JH1NPG"
      },
      "outputs": [],
      "source": [
        "[' ## '.join([word.text for word in sent.words]) for sent in stanza_nlp(paragraph).sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYjPbx0Udr2S"
      },
      "source": [
        "### [Stanza for BioNLP](https://doi.org/10.1093/jamia/ocab090)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXL8R5SNedmU"
      },
      "outputs": [],
      "source": [
        "USE_GPU = True\n",
        "stanza.download('en', package='craft')\n",
        "stanza_bionlp = stanza.Pipeline('en', package='craft', processors='tokenize', use_gpu=USE_GPU)\n",
        "# stanza.download('en', package='mimic')\n",
        "# stanza_bionlp = stanza.Pipeline('en', package='mimic', use_gpu=USE_GPU)\n",
        "# stanza_bionlp = stanza.Pipeline('en', package='mimic', processors={'ner':'i2b2'}, use_gpu=USE_GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMrPbO4VeosD"
      },
      "outputs": [],
      "source": [
        "[' ## '.join([word.text for word in sent.words]) for sent in stanza_bionlp(paragraph).sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69DOCLwloPSI"
      },
      "source": [
        "# POS tagging, Lemmatization, Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc7cltlWspi5"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "text = nltk.sent_tokenize(paragraph)[0]\n",
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7f-J-oipmJi"
      },
      "source": [
        "## [NLTK](https://www.nltk.org/api/nltk.stem.*html*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-DwOAMNsuUw"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "!cd /usr/share/nltk_data/corpora && unzip wordnet.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRaCI5tlta5D"
      },
      "outputs": [],
      "source": [
        "# POS\n",
        "nltk_pos = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "print(nltk_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0ncLHi6tz5h"
      },
      "outputs": [],
      "source": [
        "# lemma\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "' ## '.join([wnl.lemmatize(word) for word in nltk.word_tokenize(text)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q6YucMZpkSJ"
      },
      "outputs": [],
      "source": [
        "# stem\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "ps_stemmer  = PorterStemmer()\n",
        "sb_stemmer  = PorterStemmer()\n",
        "print(' ## '.join([ps_stemmer.stem(word) for word in nltk.word_tokenize(text)]))\n",
        "print(' ## '.join([sb_stemmer.stem(word) for word in nltk.word_tokenize(text)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36fua-z8oPaV"
      },
      "source": [
        "## SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpJ99LUGy89l"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "[(word.text, word.pos_, word.lemma_) for word in list(spacy_nlp(text, disable=['entity']).sents)[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRlRPeZ_yuc7"
      },
      "source": [
        "## Stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJe2Svrq6m3i"
      },
      "outputs": [],
      "source": [
        "stanza_bionlp = stanza.Pipeline('en', package='craft', processors='tokenize,pos,lemma', use_gpu=USE_GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5YsK9L13GBW"
      },
      "outputs": [],
      "source": [
        "[(word.text, word.pos, word.lemma) for word in stanza_bionlp(text).sentences[0].words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tgCw1usCU9D"
      },
      "source": [
        "# Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr5Ig6kCDpxM"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH6VRFLlCaRg"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "nltk_stop_words = set(nltk_stopwords.words('english'))\n",
        "nltk_stopwords_filter = lambda x: x not in nltk_stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUdb59n-DGPQ"
      },
      "outputs": [],
      "source": [
        "text = nltk.sent_tokenize(paragraph)[0]\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)\n",
        "print(list(filter(nltk_stopwords_filter, tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6uVhrj9E8tq"
      },
      "outputs": [],
      "source": [
        "spacy_stop_words = spacy_nlp.Defaults.stop_words\n",
        "spacy_stopwords_filter = lambda x: x not in spacy_stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-eEx2W_FkXy"
      },
      "outputs": [],
      "source": [
        "tokens = [word.text for word in list(spacy_nlp(text, disable=['entity']).sents)[0]]\n",
        "print(tokens)\n",
        "print(list(filter(spacy_stopwords_filter, tokens)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDhGz0-u4QBA"
      },
      "source": [
        "# Name Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giLyj0D_TRlh"
      },
      "source": [
        "## API Calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwqzvWcwTV94"
      },
      "outputs": [],
      "source": [
        "!rm bionlp -rf\n",
        "!git clone https://github.com/cskyan/bionlp.git\n",
        "!pip install ftfy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3Edu1dITnxg"
      },
      "outputs": [],
      "source": [
        "from bionlp.spider import pubtator\n",
        "client = pubtator.PubTatorAPI() \n",
        "client.get_concepts_pmid(ctype='all', pmid='28483577')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW0-Y8UY-Dyf"
      },
      "source": [
        "## Pre-installed NER models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKJKYnqr_s2D"
      },
      "source": [
        "### Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77GcQypD8zpJ"
      },
      "outputs": [],
      "source": [
        "[[(ent.text, ent.start, ent.end, ent.start_char, ent.end_char, ent.label_) for ent in sent.ents] for sent in spacy_nlp(paragraph).sents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR-kgWSU_v8b"
      },
      "source": [
        "### Stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAW8w8wz-V2J"
      },
      "outputs": [],
      "source": [
        "stanza_bionlp = stanza.Pipeline('en', package='craft', processors='tokenize,pos,lemma,ner', use_gpu=USE_GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMsnnuUJ-iq2"
      },
      "outputs": [],
      "source": [
        "[[(ent.text, ent.start_char, ent.end_char, ent.type) for ent in sent.ents] for sent in stanza_nlp(paragraph).sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYixMAbcBCiV"
      },
      "source": [
        "## BLUE Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rANtpOTUnv8g"
      },
      "outputs": [],
      "source": [
        "bc5cdrdz_train = pd.read_csv('data/BC5CDR/BC5CDR-disease/train.tsv', sep='\\t', header=None)\n",
        "bc5cdrdz_dev = pd.read_csv('data/BC5CDR/BC5CDR-disease/dev.tsv', sep='\\t', header=None)\n",
        "bc5cdrdz_test = pd.read_csv('data/BC5CDR/BC5CDR-disease/test.tsv', sep='\\t', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coke-_QpzwWT"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "sep_selector_train = bc5cdrdz_train[0].apply(lambda x: True if x=='.' else False)\n",
        "sep_selector_train.iloc[-1] = False if sep_selector_train.iloc[-2] else True\n",
        "int_idx = pd.DataFrame(np.arange(bc5cdrdz_train.shape[0]), index=bc5cdrdz_train.index)\n",
        "boundaries_train = [0] + list(itertools.chain.from_iterable((int_idx[sep_selector_train.values].values+1).tolist()))\n",
        "\n",
        "sep_selector_dev = bc5cdrdz_dev[0].apply(lambda x: True if x=='.' else False)\n",
        "sep_selector_dev.iloc[-1] = False if sep_selector_dev.iloc[-2] else True\n",
        "int_idx = pd.DataFrame(np.arange(bc5cdrdz_dev.shape[0]), index=bc5cdrdz_dev.index)\n",
        "boundaries_dev = [0] + list(itertools.chain.from_iterable((int_idx[sep_selector_dev.values].values+1).tolist()))\n",
        "\n",
        "sep_selector_test = bc5cdrdz_test[0].apply(lambda x: True if x=='.' else False)\n",
        "sep_selector_test.iloc[-1] = False if sep_selector_test.iloc[-2] else True\n",
        "int_idx = pd.DataFrame(np.arange(bc5cdrdz_test.shape[0]), index=bc5cdrdz_test.index)\n",
        "boundaries_test = [0] + list(itertools.chain.from_iterable((int_idx[sep_selector_test.values].values+1).tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6QjS6Ni2Hz7"
      },
      "outputs": [],
      "source": [
        "tokens_train = [bc5cdrdz_train.iloc[boundaries_train[sent_bndry]:boundaries_train[sent_bndry+1]][0].apply(str) for sent_bndry in range(len(boundaries_train)-1)]\n",
        "tokens_dev = [bc5cdrdz_dev.iloc[boundaries_dev[sent_bndry]:boundaries_dev[sent_bndry+1]][0].apply(str) for sent_bndry in range(len(boundaries_dev)-1)]\n",
        "tokens_test = [bc5cdrdz_test.iloc[boundaries_test[sent_bndry]:boundaries_test[sent_bndry+1]][0].apply(str) for sent_bndry in range(len(boundaries_test)-1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eybETqq73r7T"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import svm\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "y = le.fit_transform(bc5cdrdz_train[3])\n",
        "\n",
        "text_clf = Pipeline([\n",
        "  ('tfidf', TfidfVectorizer(analyzer='char_wb', stop_words='english', ngram_range=(2, 2), use_idf=True)),\n",
        "  ('clf', OneVsRestClassifier(svm.SVC())),])\n",
        "text_clf.fit(bc5cdrdz_train[0].fillna('').values, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVEV8WDU67Xb"
      },
      "outputs": [],
      "source": [
        "predict_test = text_clf.predict(bc5cdrdz_test[0].fillna('').values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NwORyUH7Cp6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(le.transform(bc5cdrdz_test[3]), predict_test, target_names=le.inverse_transform(text_clf.classes_)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n5FMmgmSoD0"
      },
      "source": [
        "# Document Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFuZO-96H2O0"
      },
      "source": [
        "## Bag-of-words (N-Gram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icZVvblDMc5H"
      },
      "outputs": [],
      "source": [
        "import spacy, stanza\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def spacy_tokenizer(doc):\n",
        "  return [x.orth_ for x in spacy_nlp(doc, disable=['ner'])]\n",
        "\n",
        "def stanza_tokenizer(doc):\n",
        "  return [x.orth_ for x in nlp(doc)]\n",
        "\n",
        "TKNZR = [None, nltk.word_tokenize, spacy_tokenizer]\n",
        "tknzr = TKNZR[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2CT9kQ1M8zz"
      },
      "outputs": [],
      "source": [
        "cnt_vctrzr = CountVectorizer(analyzer='word', tokenizer=tknzr, stop_words='english', ngram_range=(2, 2))\n",
        "cnt_X = cnt_vctrzr.fit_transform(pubmed_df.text.values)\n",
        "print('Numeric Features:')\n",
        "pd.DataFrame.sparse.from_spmatrix(cnt_X, index=pubmed_df.index, columns=cnt_vctrzr.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y94W236eRKUg"
      },
      "outputs": [],
      "source": [
        "bin_vctrzr = CountVectorizer(analyzer='word', tokenizer=tknzr, stop_words='english', ngram_range=(2, 2), binary=True)\n",
        "bin_X = bin_vctrzr.fit_transform(pubmed_df.text.values)\n",
        "print('Binary Features:')\n",
        "pd.DataFrame.sparse.from_spmatrix(bin_X, index=pubmed_df.index, columns=cnt_vctrzr.get_feature_names())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olKlXZBNPRQ3"
      },
      "source": [
        "## Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o1W3HCzPV2E"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjSI8QhvOjlE"
      },
      "outputs": [],
      "source": [
        "tf_vctrzr = TfidfVectorizer(analyzer='word', tokenizer=tknzr, stop_words='english', ngram_range=(2, 2), use_idf=False)\n",
        "tf_X = tf_vctrzr.fit_transform(pubmed_df.text.values)\n",
        "print('TF Features:')\n",
        "pd.DataFrame.sparse.from_spmatrix(tf_X, index=pubmed_df.index, columns=tf_vctrzr.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWE1l8vSR99c"
      },
      "outputs": [],
      "source": [
        "tfidf_vctrzr = TfidfVectorizer(analyzer='word', tokenizer=tknzr, stop_words='english', ngram_range=(2, 2), use_idf=True)\n",
        "tfidf_X = tfidf_vctrzr.fit_transform(pubmed_df.text.values)\n",
        "print('TF-IDF Features:')\n",
        "pd.DataFrame.sparse.from_spmatrix(tfidf_X, index=pubmed_df.index, columns=tfidf_vctrzr.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1vCjCUcStQ8"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "\n",
        "clf = svm.SVC()\n",
        "\n",
        "y = [0, 1, 1, 0, 1]\n",
        "clf.fit(tfidf_X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzE8LTX1WsWk"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "# text_clf = Pipeline([\n",
        "#   ('vect', CountVectorizer(analyzer='word', tokenizer=tknzr, stop_words='english', ngram_range=(2, 2))),\n",
        "#   ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "#   ('clf', svm.SVC()),])\n",
        "\n",
        "# Equivalent to\n",
        "text_clf = Pipeline([\n",
        "  ('tfidf', TfidfVectorizer(analyzer='word', tokenizer=tknzr, stop_words='english', ngram_range=(2, 2), use_idf=True)),\n",
        "  ('clf', svm.SVC()),])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inAM6gnmUvzP"
      },
      "outputs": [],
      "source": [
        "text_clf.fit(pubmed_df.text.values, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYZsdYThbida"
      },
      "source": [
        "## Multi-class & Multi-label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4vOp6b3YitV"
      },
      "outputs": [],
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "y = [1, 2, 0, 0, 2]\n",
        "\n",
        "text_clf = Pipeline([\n",
        "  ('tfidf', TfidfVectorizer(analyzer='word', tokenizer=tknzr, stop_words='english', ngram_range=(2, 2), use_idf=True)),\n",
        "  ('clf', OneVsRestClassifier(svm.SVC())),])\n",
        "text_clf.fit(pubmed_df.text.values, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MM6Fkh1dOPn"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "Y = [[1,0,1],[0,1,0],[1,1,1],[0,0,0],[0,0,1]]\n",
        "\n",
        "text_clf = Pipeline([\n",
        "  ('tfidf', TfidfVectorizer(analyzer='word', tokenizer=tknzr, stop_words='english', ngram_range=(2, 2), use_idf=True)),\n",
        "  ('clf', RandomForestClassifier(n_estimators=10)),])\n",
        "text_clf.fit(pubmed_df.text.values, Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7nGJ9MKeVL7"
      },
      "source": [
        "## BLUE Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh1GRNlgnHwb"
      },
      "outputs": [],
      "source": [
        "chemprot_train = pd.read_csv('data/ChemProt/train.tsv', sep='\\t', index_col='index')\n",
        "chemprot_dev = pd.read_csv('data/ChemProt/dev.tsv', sep='\\t', index_col='index')\n",
        "chemprot_test = pd.read_csv('data/ChemProt/test.tsv', sep='\\t', index_col='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGW-GDvAfd5J"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "y = le.fit_transform(chemprot_train.label)\n",
        "\n",
        "text_clf = Pipeline([\n",
        "  ('tfidf', TfidfVectorizer(analyzer='word', tokenizer=tknzr, stop_words='english', ngram_range=(2, 2), use_idf=True)),\n",
        "  ('clf', OneVsRestClassifier(svm.SVC())),])\n",
        "text_clf.fit(chemprot_train.sentence.values, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htdwabAbgylL"
      },
      "outputs": [],
      "source": [
        "predict_test = text_clf.predict(chemprot_test.sentence.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYhjbbdwhK-E"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(le.transform(chemprot_test.label), predict_test, target_names=le.inverse_transform(text_clf.classes_)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb7r-AFzeVRf"
      },
      "source": [
        "# Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0G8DUfsF8s7"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt7itwSsIhBQ"
      },
      "outputs": [],
      "source": [
        "# model_path = 'PATH_TO_W2V_MODEL'\n",
        "# model = KeyedVectors.load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTrnnfJRJaGK"
      },
      "outputs": [],
      "source": [
        "# Pre-process corpus\n",
        "corpus = pubmed_df.text.values\n",
        "processed_corpus = [list(filter(nltk_stopwords_filter, nltk.word_tokenize(document))) for document in corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q-ugTL_Ls1r"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "w2v_model = Word2Vec(sentences=processed_corpus, size=100, window=5, min_count=1, workers=4)\n",
        "w2v_model.save('word2vec.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFIPF1LjHdr3"
      },
      "outputs": [],
      "source": [
        "# Load the model if it exists\n",
        "model_path = 'word2vec.model'\n",
        "w2v_model = KeyedVectors.load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyNw3Bd_M0Ik"
      },
      "outputs": [],
      "source": [
        "def word2idx(lm_model, word, inexistence=-1):\n",
        "  try:\n",
        "    idx = lm_model.wv.vocab[word.lower()].index\n",
        "  except KeyError as e:\n",
        "    # print('\\'%s\\' is not in the vocabulary!' % word)\n",
        "    return inexistence\n",
        "  else:\n",
        "    return idx\n",
        "\n",
        "\n",
        "def get_embedding_layer(lm_model, **kwargs):\n",
        "  weights = lm_model.wv.vectors\n",
        "  from keras.layers import Embedding\n",
        "  from keras.initializers import Constant\n",
        "  return Embedding(input_dim=weights.shape[0], output_dim=weights.shape[1], weights=[weights], **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GR2UTIK5Bbv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "\n",
        "# Construct the model\n",
        "X_inputs = Input(shape=(128,), dtype='int64', name='X')\n",
        "embd_layer = get_embedding_layer(w2v_model)\n",
        "lstm_layer = LSTM(128, name='LSTM')(embd_layer(X_inputs))\n",
        "hidden_state = Dense(32, activation='relu', name='Hidden-State')(lstm_layer)\n",
        "output = Dropout(0.2, name='Dropout')(Dense(1, activation='relu', name='CLF')(hidden_state))\n",
        "\n",
        "# Compile the model\n",
        "model = Model(X_inputs, output)\n",
        "optmzr = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=optmzr, loss='binary_crossentropy', metrics=['acc', 'mse'])\n",
        "\n",
        "# Fed inputs\n",
        "input_seqs = [[word2idx(w2v_model, word, inexistence=len(w2v_model.wv.vectors)-1) for word in doc] for doc in processed_corpus]\n",
        "input_ids = sequence.pad_sequences(input_seqs, maxlen=128, dtype='int64', padding='post', truncating='post', value=len(w2v_model.wv.vectors)-1)\n",
        "y = np.array([0, 1, 1, 0, 1])\n",
        "model.fit(input_ids, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPPI5IX8hbfS"
      },
      "source": [
        "## Execercise: Use word embedding for the BLUE classification tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdOIk3KDhv1I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_ZlqV4enKZY"
      },
      "source": [
        "# Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc2-d0RZF9ds"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\" -O biobert_weights && rm -rf /tmp/cookies.txt\n",
        "!tar -xzf biobert_weights\n",
        "!transformers-cli convert --model_type bert --tf_checkpoint biobert_v1.1_pubmed/model.ckpt-1000000 --config biobert_v1.1_pubmed/bert_config.json --pytorch_dump_output biobert_v1.1_pubmed/pytorch_model.bin\n",
        "!mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFh7gK0NQmuU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertConfig, BertTokenizer, BertModel, AutoTokenizer, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78q3LKiaje8X"
      },
      "outputs": [],
      "source": [
        "# Load models\n",
        "tokenizer = AutoTokenizer.from_pretrained('biobert_v1.1_pubmed')\n",
        "model = BertForSequenceClassification.from_pretrained('biobert_v1.1_pubmed')\n",
        "loss_func = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3Otwe2lmIpm"
      },
      "outputs": [],
      "source": [
        "# Construct inputs and get outputs\n",
        "inputs = tokenizer(pubmed_df.text.values.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "output = model(**inputs)\n",
        "logits = output.logits\n",
        "# predictions = F.softmax(output.logits, -1).argmax(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeYOE_PVpeP3"
      },
      "outputs": [],
      "source": [
        "# Calculate the loss and backward propergate for training\n",
        "y = torch.tensor([0, 1, 1, 0, 1])\n",
        "loss = loss_func(logits, y)\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJRbZHEPh53q"
      },
      "source": [
        "## Execercise: Use BERT for BLUE NER tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wszbDdIniKCX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJiXhbD0ZRMV"
      },
      "source": [
        "# The End"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oH9M0MS0FKWt",
        "t1Wgd63gQxEd",
        "1PKeNac8Y1q-",
        "b3iqsfDm62zv",
        "yDbkBt6b7RSd",
        "gR4GYytf9OSD",
        "w3vfNYE0MAQQ",
        "mqffDwwlYLaE",
        "7nCRhFHfMNZ4",
        "le1azQ7qPw8S",
        "O_12cfUMZRGR",
        "27BO5ZWgdYFt",
        "77OBNmfYcD7W",
        "sYjPbx0Udr2S",
        "69DOCLwloPSI",
        "j7f-J-oipmJi",
        "36fua-z8oPaV",
        "iRlRPeZ_yuc7",
        "3tgCw1usCU9D",
        "lDhGz0-u4QBA",
        "giLyj0D_TRlh",
        "eW0-Y8UY-Dyf",
        "AKJKYnqr_s2D",
        "DR-kgWSU_v8b",
        "cYixMAbcBCiV",
        "2n5FMmgmSoD0",
        "cFuZO-96H2O0",
        "olKlXZBNPRQ3",
        "qYZsdYThbida",
        "X7nGJ9MKeVL7",
        "rb7r-AFzeVRf",
        "WPPI5IX8hbfS",
        "f_ZlqV4enKZY",
        "WJRbZHEPh53q",
        "OJiXhbD0ZRMV"
      ],
      "name": "bionlp_intro.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
